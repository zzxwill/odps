# 数据上传下载概述 {#concept_am2_p3f_vdb .concept}

本文为您概述如何将数据上传至MaxCompute或将数据从MaxCompute下载，具体包括服务连接、SDK、工具和数据上云场景等常见操作。

总的来说，您可以通过DataHub实时数据通道和Tunnel批量数据通道两种途径进出MaxCompute系统。DataHub和Tunnel各自都提供了SDK，而基于这些SDK又衍生了许多用于数据上传/下载的工具，方便您在各种场景下的数据进行上传/下载的需求。

数据上传/下载的工具主要包括：DataWorks、DTS、OGG插件、Sqoop、Flume插件、LogStash插件、Fluentd插件、Kettle插件以及MaxCompute客户端等。

上述工具使用的底层数据通道，分类如下：

-   DataHub通道系列
    -   OGG插件
    -   Flume插件
    -   LogStash插件
    -   Fluentd插件
-   Tunnel通道系列
    -   DataWorks
    -   DTS
    -   Sqoop
    -   Kettle插件
    -   MaxCompute客户端

基于上述丰富的数据上传/下载的工具，可以满足大部分常见的数据上云场景，后续的章节会对工具本身以及[Hadoop数据迁移](../../../../intl.zh-CN/最佳实践/Hadoop数据迁移MaxCompute最佳实践.md#)、数据库数据同步、[日志采集](intl.zh-CN/用户指南/数据上传下载/DataHub实时数据通道.md#)等数据上云的场景进行介绍，为您进行技术方案选型时提供参考。

**说明：** 对于离线数据的同步，推荐您优先使用[数据集成](../../../../intl.zh-CN/使用指南/数据集成/数据集成简介/数据集成概述.md#)。

## 使用限制 {#section_epm_d5m_sfb .section}

Tunnel Upload命令上传限制

-   上传不设速度限制，上传速度的瓶颈在网络带宽以及服务器性能。
-   重传retry有次数的限制的，当重传的次数超过了这个限制，就会继续上传下一个block。上传完成后，可以通过`select count(*)`语句，检查是否有数据丢失。
-   Tunnel命令不支持上传下载Array、Map和Struct类型的数据。
-   每个Tunnel的Session在服务端的生命周期为24小时，创建后24小时内均可使用，也可以跨进程/线程共享使用，但是必须保证同一个BlockId没有重复使用。

DataHub上传数据限制

-   每个字段的大小不能超过这个字段本身的限制，详情请参见[数据类型](intl.zh-CN/用户指南/基本概念/数据类型.md#)，要特别注意String的长度不能超过8M。
-   目前上传的过程中，是多条数据打包成一个Package来进行上传。

TableTunnel SDK接口限制

-   block id的取值范围是\[0, 20000\)，单个block上传的数据限制为100G。
-   session的超时时间为24小时。如果，大批量数据传送导致超过24小时，需要自行拆分成多个session。
-   RecordWriter对应的HTTP Request超时为120s。如果120s内HTTP连接上没有数据流过，service端会主动关闭连接。

